{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["%%<br>\n", "mports"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os, sys, json, re  # Provides OS-dependent functionality, system-specific parameters, JSON handling\n", "import pandas as pd             # Provides data structures and data analysis tools\n", "import numpy as np              # Supports large, multi-dimensional arrays and matrices\n", "import requests\n", "import time\n", "import xlsxwriter\n", "from tqdm import tqdm\n", "from datetime import date #date/time manipulation\n", "import lxml"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import urllib3\n", "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n", "import warnings\n", "warnings.simplefilter(action='ignore', category=FutureWarning)\n", "pd.options.mode.chained_assignment = None  # default='warn'\n", "from IPython.display import display_markdown"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Import requests for fetching the web page"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import requests"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Import BeautifulSoup for parsing HTML"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from bs4 import BeautifulSoup,SoupStrainer"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from io import StringIO"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["soup_url = r'https://ballotpedia.org/State_Legislative_Districts'\n", "# %%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["all_districts = []\n", "response = requests.get(soup_url, verify = False)\n", "soup = BeautifulSoup(response.content, 'html.parser')\n", "state_districts = soup.find_all(\"a\", href = True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["h_refs = []\n", "for url in state_districts:\n", "    if 'state legislative districts' in str(url):\n", "        # print(url)\n", "        base = \"https://ballotpedia.org/\"\n", "        full_url = base + str(url.text).replace(' ',\"_\")\n", "        h_refs.append(full_url)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["print(*h_refs, sep=\"\\n\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for ref in h_refs:\n", "    url = ref\n", "    \n", "    page = requests.get(url)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    \n", "    os.chdir(r'C:\\Users\\clutz\\OneDrive - THE HUNT INSTITUTE\\Documents\\Data\\txt files for troubleshooting')\n\n", "    # Write the page's text content to a file\n", "    # with open('output_soup_strainer.txt', \"w\", encoding=\"utf-8\") as f:\n", "    #     f.write(page.text)\n", "    # print(page.content)\n", "    table_strainer = SoupStrainer('table', id='officeholder-table')\n", "    page_soup = BeautifulSoup(page.content, 'html.parser', parse_only=table_strainer)\n\n", "    # print(page_soup.content)\n", "    # print(type(page_soup))\n", "    districts = page_soup.find_all(\"a\")\n", "    total_districts = []\n", "    # print(page_soup.prettify())\n", "    for d in districts:\n", "        total_districts.append(d.text)\n", "        # print(d.text)\n", "    all_districts.extend(total_districts)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Setting up regex patterns to look for states in other columns"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ull state names"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["state_list = [\n", "    \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \n", "    \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \n", "    \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \n", "    \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \n", "    \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \n", "    \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \n", "    \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \n", "    \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \n", "    \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \n", "    \"West Virginia\", \"Wisconsin\", \"Wyoming\", \"District of Columbia\"\n", "]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["tate abbreviations"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["state_abbreviations = [\n", "    \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\", \n", "    \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \n", "    \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \n", "    \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \n", "    \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\", \"DC\"\n", "]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["reation of specific regex to look for state at the beginning of the string"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["state_abbreviations_reg = []\n", "for abv in state_abbreviations:\n", "    for_regex = f'^{abv}'\n", "    state_abbreviations_reg.append(for_regex)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["ompiling regex patterns for looking for states"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["state_pat = re.compile(\"|\".join(state_list))\n", "state_abv_pat = re.compile(\"|\".join(state_abbreviations_reg))\n", "#dictionary creation for future reference in later cells\n", "state_ref = dict(zip(state_list, state_abbreviations))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["codes = list(range(10,61,1))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["state_coding = dict(zip(state_abbreviations, codes))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["thi_states = ['ND', 'NM', 'OH', 'OK', 'VA', 'WV', 'AL', 'CT', 'IL', 'IN', 'KS', 'MO', 'NC']"]}, {"cell_type": "markdown", "metadata": {}, "source": ["for k,v in state_coding.items():<br>\n", "    print(k)<br>\n", "    print(v)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["state_intitals = []\n", "for state in all_districts:\n", "    state_match = re.findall(state_pat, state)[0]\n", "    ref_lookup = state_ref.get(str(state_match))\n", "    state_intitals.append(ref_lookup)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["districts_w_intials = pd.DataFrame({'state_abbreviation': state_intitals,'district_string': all_districts})\n", "thi_state_districts = districts_w_intials[districts_w_intials['state_abbreviation'].isin(thi_states)]\n", "thi_state_districts.reset_index(inplace=True, drop=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%splitting up district numbers from rest of string"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lengths = []\n", "thi_state_districts['state_code'] = np.nan\n", "thi_state_districts['chamber_code'] = np.nan\n", "thi_state_districts['district'] = np.nan\n", "thi_state_districts['primary_key'] = np.nan\n", "for i,j in enumerate(thi_state_districts['district_string']):\n", "    # print(str(j))\n", "    # print(row)\n", "    district_raw = re.split(r'\\s(?=District)', str(j))\n", "    match = re.findall(r'\\s\\d+', str(district_raw))[0]\n", "    match = match.strip()\n", "    if len(match) == 2:\n", "        district_code = '0' + str(match)\n", "    elif len(match) == 1:\n", "        district_code = '00'+str(match)\n", "    else:\n", "        district_code = str(match)\n", "    district_len = len(match)\n", "    lengths.append(district_len)\n", "    ext_state = thi_state_districts.loc[i,'state_abbreviation']\n", "    state_code = state_coding.get(ext_state)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    if 'house' in str(j).lower():\n", "        chamber_code = '0'\n", "    elif 'senate' in str(j).lower():\n", "        chamber_code = '1'\n", "    else:\n", "        print(f'unknown chamber: {str(j)}')\n", "        break\n", "    \n", "    # display_markdown(f'#### {ext_state} - {chamber_code} - {district_raw}', raw=True)\n", "    key_code = f'{state_code}{chamber_code}{district_code}'\n", "    \n", "    \n", "    thi_state_districts.loc[i,'state_code'] = state_code\n", "    thi_state_districts.loc[i,'chamber_code'] = chamber_code\n", "    thi_state_districts.loc[i,'district'] = match\n", "    thi_state_districts.loc[i,'primary_key'] = key_code"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    \n", "    \n", "# %% takes duplicates of primary keys and assigns a seat num"]}, {"cell_type": "markdown", "metadata": {}, "source": ["eat num of 00 means there is only one seat (no multi-member districts)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["thi_state_districts['count'] = thi_state_districts['primary_key'].map(thi_state_districts['primary_key'].value_counts())\n", "thi_state_districts['seat_num'] = thi_state_districts.groupby('primary_key').cumcount() + 1\n", "thi_state_districts.loc[thi_state_districts['count'] == 1, 'seat_num'] = 0\n", "thi_state_districts = thi_state_districts.drop(columns='count')\n", "thi_state_districts['seat_num'] = thi_state_districts['seat_num'].apply(lambda x: f'{x:02d}')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["thi_state_districts['final_primary_key'] = thi_state_districts['primary_key'] + thi_state_districts['seat_num']\n", "# %%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["duplicate = thi_state_districts[thi_state_districts.duplicated('primary_key')]\n\n", "    # print(str(j))\n", "    # print(district_len)\n", "    "]}, {"cell_type": "markdown", "metadata": {}, "source": ["print(max(lengths))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    # print(line.split('District')[-1])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(*all_districts, sep='\\n')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["[print(x) for x in thi_state_districts]\n\n", "    # states.append(str(state_match))\n", "# %%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["states = sorted(list(set(states)))\n", "print(len(states))\n", "print(*states, sep=\"\\n\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for i,j in enumerate(states):\n", "    code = str(i+1)\n", "    if len(code) == 1:\n", "        code = \"0\"+code\n", "    df = pd.DataFrame({'state': [j], 'code':[code]})\n", "    print(df.to_string())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    \n", "    break\n", "    print(page.status_code)\n", "    print(page.content[:500])  # Preview the content\n", "    \n", "    # df_list = pd.read_html(page_soup.prettify())\n", "    # print(len(df_list))\n", "    # print(type(df_list))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    # print(df)\n\n", "    # dis_soup = BeautifulSoup(response.content, 'html.parser')\n", "    # ditricts_tags = dis_soup.find_all(\"td\")\n", "    # for d in ditricts_tags:\n", "    #     print(d)\n", "    \n", "\t"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%%"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}