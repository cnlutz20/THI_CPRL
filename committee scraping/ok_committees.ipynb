{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, datetime, re  # Provides OS-dependent functionality, system-specific parameters, JSON handling, and date/time manipulation\n",
    "import pandas as pd             # Provides data structures and data analysis tools\n",
    "import numpy as np              # Supports large, multi-dimensional arrays and matrices\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree, html\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from cprl_functions.state_capture import thi_states,state_ref, state_coding, state_pat, state_abv_pat\n",
    "from cprl_functions.text_printing import bordered\n",
    "from cprl_functions.defined_functions import create_pk, extract_title_and_name, get_recent_file\n",
    "from unidecode import unidecode\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Driver Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "webdriver_path = r\"C:\\Users\\clutz\\hunt_env\\chrome driver\\chromedriver-win64\\chromedriver.exe\"\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument('--headless')\n",
    "# chrome_options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "# Set up WebDriver service\n",
    "service = Service(webdriver_path)\n",
    "\n",
    "#call on driver\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=chrome_options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "senate: https://oksenate.gov/committees-list\n",
      "\n",
      "house: https://www.okhouse.gov/committees/house\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get data\n",
    "senate_page_url = r'https://oksenate.gov/committees-list'\n",
    "house_page_url = r'https://www.okhouse.gov/committees/house'\n",
    "\n",
    "urls = pd.DataFrame({\"chamber\":['senate', 'house'],\"urls\":[senate_page_url,house_page_url]}).reset_index(drop = True)\n",
    "urls_dict = urls.set_index('chamber')['urls'].to_dict()\n",
    "\n",
    "for k,u in urls_dict.items():\n",
    "    print(f'{k}: {u}{\"\\n\"}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appropriations\n"
     ]
    }
   ],
   "source": [
    "driver.get(\"https://oksenate.gov/committees/appropriations\")\n",
    "# time.sleep(2)\n",
    "\n",
    "# class=\"field field--name-title field--type-string field--label-hidden\"\n",
    "\n",
    "title = driver.find_element(By.XPATH,'//span[@class=\"field field--name-title field--type-string field--label-hidden\"]')\n",
    "print(title.text)\n",
    "if re.search(title.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## House"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "com name: A&B Education Subcommittee\n",
      "not a relevant comm: A&B Finance Subcommittee\n",
      "not a relevant comm: A&B General Government Subcommittee\n",
      "com name: A&B Health Subcommittee\n",
      "not a relevant comm: A&B Human Services Subcommittee\n",
      "not a relevant comm: A&B Judiciary Subcommittee\n",
      "not a relevant comm: A&B Natural Resources Subcommittee\n",
      "not a relevant comm: A&B Public Safety Subcommittee\n",
      "not a relevant comm: A&B Select Agencies Subcommittee\n",
      "not a relevant comm: A&B Transportation Subcommittee\n",
      "5\n",
      "not a relevant comm: Business\n",
      "not a relevant comm: Government Modernization and Technology\n",
      "not a relevant comm: Insurance\n",
      "not a relevant comm: Tourism\n",
      "not a relevant comm: Transportation\n",
      "2\n",
      "com name: Common Education\n",
      "com name: Postsecondary Education\n",
      "4\n",
      "not a relevant comm: Agriculture\n",
      "not a relevant comm: Utilities\n",
      "not a relevant comm: Wildlife\n",
      "not a relevant comm: Energy\n",
      "5\n",
      "not a relevant comm: County and Municipal Government\n",
      "not a relevant comm: Elections and Ethics\n",
      "not a relevant comm: General Government\n",
      "not a relevant comm: Banking, Financial Services and Pensions\n",
      "not a relevant comm: State Powers\n",
      "4\n",
      "com name: Public Health\n",
      "not a relevant comm: Alcohol, Tobacco and Controlled Substances\n",
      "com name: Children, Youth and Family Services\n",
      "not a relevant comm: Veteran and Military Affairs\n",
      "3\n",
      "not a relevant comm: Civil Judiciary\n",
      "not a relevant comm: Criminal Judiciary\n",
      "not a relevant comm: Public Safety\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "house_links = []\n",
    "\n",
    "comms_dict = {}\n",
    "for k,u in urls_dict.items():\n",
    "    if k.lower().strip() == \"house\":\n",
    "        driver.get(u)\n",
    "\n",
    "        html_from_page = driver.page_source\n",
    "        soup = BeautifulSoup(html_from_page, 'html.parser')\n",
    "        links = soup.find_all(\"a\", {\"class\": \"theme-shape theme-border relative flex flex-nowrap py-4 px-6 items-center border border-warmgray-300 shadow-md bg-white hover:shadow-xl min-h-[120px]\"}, href=True)\n",
    "        \n",
    "        for link in links:\n",
    "            link_dl = f'https://www.okhouse.gov/{link.attrs.get(\"href\")}'\n",
    "            driver.get(link_dl)\n",
    "            time.sleep(2)\n",
    "            subs = link.get_attribute\n",
    "            html_from_page = driver.page_source\n",
    "            sub_soup = BeautifulSoup(html_from_page, 'html.parser')\n",
    "            \n",
    "            subs = sub_soup.find_all(\"a\", {'class':'cursor-pointer text-primary underline'}, href = True)\n",
    "            print(len(subs))\n",
    "            \n",
    "            for sub in subs:\n",
    "                \n",
    "                keywords = ['[Ee]ducation', '[Cc]hildren', '[Yy]oung', '[Hh]ealth']\n",
    "                pat = re.compile(\"|\".join(keywords))\n",
    "                if re.search(pat, sub.text):\n",
    "                    print(f'com name: {sub.text}')\n",
    "                else:\n",
    "                    print(f'not a relevant comm: {sub.text}')\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                sub_link = f'https://www.okhouse.gov/{sub['href']}'\n",
    "                sub_name = sub.text\n",
    "                comms_dict[sub_name] = sub_link\n",
    "\n",
    "\n",
    "                # print(f'sub: {sub}')\n",
    "            # house_links.append(link_dl)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A&B Education Subcommittee: https://www.okhouse.gov//committees/house/approp/ap-edu\n",
      "A&B Health Subcommittee: https://www.okhouse.gov//committees/house/approp/ap-hlth\n",
      "Common Education: https://www.okhouse.gov//committees/house/edu/comed\n",
      "Postsecondary Education: https://www.okhouse.gov//committees/house/edu/posted\n",
      "Public Health: https://www.okhouse.gov//committees/house/hhs/pubhlth\n",
      "Children, Youth and Family Services: https://www.okhouse.gov//committees/house/hhs/child\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# //*[@id=\"__next\"]/div/div[2]/div/div[1]/div[2]/div/h2\n",
    "for k,v in comms_dict.items():\n",
    "    print(f'{k}: {v}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_dict = {}\n",
    "for k,lk in comms_dict.items():\n",
    "    driver.get(lk)\n",
    "\n",
    "\n",
    "    # html_from_page = driver.page_source\n",
    "    # soup = BeautifulSoup(html_from_page, 'html.parser')\n",
    "    # <h1 class=\"brand-font text-center type-h2 mb-4\">Health and Human Services Oversight</h1>\n",
    "    time.sleep(2)\n",
    "    com_name = driver.find_element(By.XPATH, \"//h1[@class='brand-font text-center type-h2 mb-4']\")\n",
    "    com_name = com_name.text\n",
    "    keywords = ['[Ee]ducation', '[Cc]hildren', '[Yy]oung']\n",
    "    pat = re.compile(\"|\".join(keywords))\n",
    "    if re.search(pat, com_name):\n",
    "        print(f'com name: {com_name}')\n",
    "    else:\n",
    "        print(f'not a relevant comm: {com_name}')\n",
    "        continue\n",
    "\n",
    "\n",
    "    elements = driver.find_elements(By.XPATH, \"//article[starts-with(@aria-label, 'see details on')]\")\n",
    "    \n",
    "\n",
    "    # leg_arts = soup.find_all(\"article\")\n",
    "    dfs = []\n",
    "    for leg in elements:\n",
    "        # print(leg.get_attribute('outerHTML'))\n",
    "        print('\\n')\n",
    "\n",
    "        # print(type(leg))\n",
    "        leg_name = leg.find_element(By.XPATH, \".//p[@class = 'text-primary cta utility-font mb-1']\")\n",
    "        leg_district = leg.find_element(By.XPATH, \".//p[@class = 'ml-2 utility-font label']\")\n",
    "        try:\n",
    "            leg_position = leg.find_element(By.XPATH, \".//p[@class = 'utility-font caption mb-1']\")\n",
    "            leg_position = leg.text\n",
    "        except:\n",
    "            leg_position = np.nan\n",
    "            print('not a comm head')\n",
    "        \n",
    "        \n",
    "        df = pd.DataFrame({'name': [leg_name.text], 'position': [leg_position], 'district': [leg_district.text]})\n",
    "        dfs.append(df)\n",
    "    \n",
    "    leg_data = pd.concat(dfs).reset_index(drop=True)\n",
    "    print(f'length: {len(leg_data)}')\n",
    "        # print(leg_name.text)\n",
    "        # print(leg_name.get_attribute('outerHTML'))\n",
    "    comm_dict[com_name] = leg_data\n",
    "    \n",
    "    # name = <p class=\"text-primary cta utility-font mb-1\">Brian Hill</p>\n",
    "    # position = <p class=\"utility-font caption mb-1\">Chair</p>\n",
    "    # district = <p class=\"ml-2 utility-font label\">District 47</p> #only for chair and vice chair\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A&B Education Subcommittee:               name                                    position     district\n",
      "0    CHAD CALDWELL        Chair\\nCHAD CALDWELL\\nR\\nDistrict 40  District 40\n",
      "1   TONI HASENBECK  Vice Chair\\nTONI HASENBECK\\nR\\nDistrict 65  District 65\n",
      "2    CHRIS BANNING                                         NaN  District 24\n",
      "3      RONNY JOHNS                                         NaN  District 25\n",
      "4        DICK LOWE                                         NaN  District 56\n",
      "5  MICHELLE MCCANE                                         NaN  District 72\n",
      "6      MIKE OSBURN                                         NaN  District 81\n",
      "7     MARK TEDFORD                                         NaN  District 69\n",
      "8     JOHN WALDRON                                         NaN  District 77\n",
      "9     GABE WOOLLEY                                         NaN  District 98\n",
      "Common Education:                 name                                    position     district\n",
      "0          DICK LOWE            Chair\\nDICK LOWE\\nR\\nDistrict 56  District 56\n",
      "1     DANNY STERLING  Vice Chair\\nDANNY STERLING\\nR\\nDistrict 27  District 27\n",
      "2      CHRIS BANNING                                         NaN  District 24\n",
      "3      CHAD CALDWELL                                         NaN  District 40\n",
      "4           ROB HALL                                         NaN  District 67\n",
      "5      MOLLY JENKINS                                         NaN  District 33\n",
      "6        RONNY JOHNS                                         NaN  District 25\n",
      "7       CODY MAYNARD                                         NaN  District 21\n",
      "8   ELLEN POGEMILLER                                         NaN  District 88\n",
      "9   JACOB ROSECRANTS                                         NaN  District 46\n",
      "10      MARK TEDFORD                                         NaN  District 69\n",
      "Postsecondary Education:               name                                 position     district\n",
      "0   TONI HASENBECK    Chair\\nTONI HASENBECK\\nR\\nDistrict 65  District 65\n",
      "1      MAX WOLFLEY  Vice Chair\\nMAX WOLFLEY\\nR\\nDistrict 95  District 95\n",
      "2    STEVE BASHORE                                      NaN   District 7\n",
      "3      RONNY JOHNS                                      NaN  District 25\n",
      "4  MICHELLE MCCANE                                      NaN  District 72\n",
      "5      MIKE OSBURN                                      NaN  District 81\n",
      "6     TRISH RANSON                                      NaN  District 34\n",
      "7    TAMMY TOWNLEY                                      NaN  District 48\n",
      "Children, Youth and Family Services:                name                                position     district\n",
      "0    DANNY WILLIAMS   Chair\\nDANNY WILLIAMS\\nR\\nDistrict 28  District 28\n",
      "1        DANIEL PAE  Vice Chair\\nDANIEL PAE\\nR\\nDistrict 62  District 62\n",
      "2  MELOYDE BLANCETT                                     NaN  District 78\n",
      "3        EMILY GISE                                     NaN  District 90\n",
      "4        BRIAN HILL                                     NaN  District 47\n",
      "5      GABE WOOLLEY                                     NaN  District 98\n"
     ]
    }
   ],
   "source": [
    "for k,v in comm_dict.items():\n",
    "    \n",
    "    print(f'{k}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_url = r\"https://www.ncleg.gov\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title was : Agriculture and Environment\n",
      "title was : Alcoholic Beverage Control\n",
      "title was : Appropriations\n",
      "title was : Appropriations, Agriculture and Natural and Economic Resources\n",
      "title was : Appropriations, Capital and Information Technology\n",
      "title was : Appropriations, General Government\n",
      "title was : Appropriations, Justice and Public Safety\n",
      "title was : Appropriations, Transportation\n",
      "title was : Commerce and Economic Development\n",
      "title was : Election Law\n",
      "title was : Emergency Management and Disaster Recovery\n",
      "title was : Energy and Public Utilities\n",
      "title was : Ethics\n",
      "title was : Federal Relations and American Indian Affairs\n",
      "title was : Finance\n",
      "title was : Homeland Security and Military and Veterans Affairs\n",
      "title was : Housing and Development\n",
      "title was : Insurance\n",
      "title was : Judiciary 1\n",
      "title was : Judiciary 2\n",
      "title was : Judiciary 3\n",
      "title was : Oversight\n",
      "title was : Pensions and Retirement\n",
      "title was : Regulatory Reform\n",
      "title was : Rules, Calendar, and Operations of the House\n",
      "title was : State and Local Government\n",
      "title was : Transportation\n",
      "title was : Wildlife Resources\n",
      "title was : Agriculture, Energy, and Environment\n",
      "title was : Appropriations on Agriculture, Natural, and Economic Resources\n",
      "title was : Appropriations on Department of Transportation\n",
      "title was : Appropriations on General Government and Information Technology\n",
      "title was : Appropriations on Justice and Public Safety\n",
      "title was : Appropriations/Base Budget\n",
      "title was : Commerce and Insurance\n",
      "title was : Elections\n",
      "title was : Finance\n",
      "title was : Judiciary\n",
      "title was : Pensions and Retirement and Aging\n",
      "title was : Regulatory Reform\n",
      "title was : Rules and Operations of the Senate\n",
      "title was : State and Local Government\n",
      "title was : Transportation\n"
     ]
    }
   ],
   "source": [
    "import fnmatch\n",
    "\n",
    "\n",
    "hrefs = {}\n",
    "for l in links:\n",
    "    if 'NonStanding' in str(l):\n",
    "        continue\n",
    "    elif 'Standing' in str(l):\n",
    "        half_link = str(l).split('href=\"', 1)[-1].split('\">', 1)[0].strip()\n",
    "        link = f'https://www.ncleg.gov{half_link}'\n",
    "        title = str(l).split('title\">', 1)[-1].split(\"</span\",1)[0].strip()\n",
    "        keywords = [\"[Hh]ealth\", \"[Ee]ducation\"]\n",
    "        pattern = re.compile('|'.join(keywords))\n",
    "        # print(pattern)\n",
    "        res = bool(re.search(pattern, title))  # Use re.search() to match anywhere in the string\n",
    "        # print(res)\n",
    "        if res is True:\n",
    "            hrefs[title] = link\n",
    "        else:\n",
    "            print(f'title was : {title}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THI Leg Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['full_pk', 'primary_key', 'district_code', 'state abbreviation',\n",
      "       'chamber', 'title', 'first name', 'last name', 'party', 'district',\n",
      "       'date assumed office', 'name', 'tenure', 'leader', 'state_code',\n",
      "       'chamber_code'],\n",
      "      dtype='object')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#set up\n",
    "ref_path = r\"C:\\Users\\clutz\\THE HUNT INSTITUTE\\The Hunt Institute Team Site - Documents\\Development (formerly Grants Management)\\!Administrative\\Christian\\THII\\legislator data\\key_creation\\2025\"\n",
    "leg_ref =pd.read_excel(get_recent_file(\"*\", ref_path))\n",
    "leg_ref = leg_ref[leg_ref['state abbreviation'] == \"NC\"]\n",
    "print(leg_ref.columns)\n",
    "print()\n",
    "#loop_group creation (last_names associated with key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#groupby data to get primary key and last names associated with it\n",
    "leg_lname_counts = leg_ref.groupby(['full_pk'])['last name'].nunique().reset_index()\n",
    "print(leg_lname_counts.columns)\n",
    "\n",
    "leg_lname_counts = leg_lname_counts[leg_lname_counts['last name']>1].reset_index(drop=True)\n",
    "print(len(leg_lname_counts))\n",
    "\n",
    "if len(leg_lname_counts) == 0:\n",
    "    print(True)\n",
    "\n",
    "leg_ref['full_pk'] = leg_ref['full_pk'].fillna(0).astype(int)\n",
    "\n",
    "#set up dict for lookup\n",
    "leg_ref_dict = leg_ref.set_index('last name')['full_pk'].to_dict()\n",
    "for name, data in leg_ref_dict.items():\n",
    "    if \"Biggs\" in name:\n",
    "        print(name)\n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#refining to only members\n",
    "members_dict = {}\n",
    "for title,com_url in hrefs.items():\n",
    "    \n",
    "    \n",
    "    driver.get(com_url)\n",
    "    html_from_page = driver.page_source\n",
    "    com_soup = BeautifulSoup(html_from_page, 'html.parser')\n",
    "\n",
    "    members = com_soup.find_all(\"a\", href=True)\n",
    "    print('#')\n",
    "\n",
    "    \n",
    "    acceptable = []\n",
    "    # [print(x.text) for x in members]\n",
    "    for mem in members:\n",
    "        search_str = str(mem).lower().strip()\n",
    "        \n",
    "        if re.search(r'members\\/bio.+\\n<img', search_str):\n",
    "            acceptable.append(mem)\n",
    "    \n",
    "    members_dict[title] = acceptable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bio dict creation\n",
    "bios_dfs = []\n",
    "names = []\n",
    "bios = []\n",
    "for title,ls in members_dict.items():\n",
    "    \n",
    "    #look through member bs4 tags\n",
    "    for l in ls:\n",
    "        name = l.text.strip()\n",
    "        names.append(name)\n",
    "        bio_url = main_url + l.attrs.get('href')\n",
    "        bios.append(bio_url)\n",
    "\n",
    "#create dict\n",
    "bio_dict = dict(zip(names,bios))\n",
    "\n",
    "# for key, value in bio_dict.items():\n",
    "#     print(f\"{key}: {value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make bio dict\n",
    "\n",
    "comm_info_dict = {}\n",
    "for title, com_members in members_dict.items():\n",
    "    #go through members list\n",
    "    dfs_concat = []\n",
    "    for mem in com_members:\n",
    "        \n",
    "        #get name\n",
    "        name = re.sub('\\n','',str(mem.text).strip())\n",
    "        leg_titles_exp = name\n",
    "\n",
    "        #get position\n",
    "        parents = mem.find_parents(limit=2)\n",
    "        for x in parents:\n",
    "            # print('###')\n",
    "            # next_siblings = x.find_next_siblings()\n",
    "            # print(x.name)\n",
    "            # print(x.attrs)\n",
    "            f_class_o_parent = x.attrs.get('class')\n",
    "            matches_parent = False\n",
    "            for f in f_class_o_parent:\n",
    "                if 'row' in str(f).lower():\n",
    "                    matches_parent = True\n",
    "            if matches_parent == True:\n",
    "                header_tag_div = x\n",
    "                break\n",
    "            \n",
    "        position_exp = header_tag_div.find_previous_sibling().text\n",
    "\n",
    "        \n",
    "        # get link to leg bio\n",
    "        bio_link = f'https://www.ncleg.gov{mem.attrs.get('href')}'\n",
    "        \n",
    "        #df creation\n",
    "        df_exp = pd.DataFrame({\"com\":[title],\"name\":[leg_titles_exp], \"position\": [position_exp], \"bio_link\": [bio_link]})\n",
    "        dfs_concat.append(df_exp)\n",
    "\n",
    "        \n",
    "    #pull dfs for com together\n",
    "    com_df = pd.concat(dfs_concat).reset_index(drop = True)\n",
    "    comm_info_dict[title] = com_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign fpk\n",
    "for k,v in comm_info_dict.items():\n",
    "    \n",
    "    v['full_pk'] = np.nan \n",
    "\n",
    "    #individual com df's\n",
    "    for i,j in enumerate(v['name']):\n",
    "        name = re.split(r'^[Rr]ep.|^[Ss]en.', str(j), maxsplit=1)[-1].strip()\n",
    "\n",
    "\n",
    "        # if there is an initial remove and cache\n",
    "        if re.search(r'^[A-Za-z]{1}\\.\\s', name):\n",
    "            name_split = re.split(r'\\.\\s', name, maxsplit=1)\n",
    "            name = name_split[-1]\n",
    "            initial = name_split[0]\n",
    "        \n",
    "        #gets all matches for the last name\n",
    "        name = unidecode(name)\n",
    "        check = leg_ref[leg_ref['last name'].str.contains(name)].reset_index(drop=True)\n",
    "        \n",
    "        print('=========================================')\n",
    "        print(f'name: {name}')\n",
    "        \n",
    "        print('#')\n",
    "        #only one match go ahead and assign\n",
    "        if len(check) == 1:\n",
    "            printit = False\n",
    "            full_pk = str(int(check.loc[0,'full_pk']))\n",
    "            v.loc[i,'full_pk'] = full_pk\n",
    "            if printit == True:\n",
    "                print(\"intial route 1: matched 1\")\n",
    "                print('_________________')            \n",
    "            # print(check.to_string())\n",
    "\n",
    "        #more than one, check intials\n",
    "        elif len(check) > 1:\n",
    "            printit = False\n",
    "\n",
    "            if printit == True:\n",
    "                print(\"intial route 2: >1 result\")\n",
    "            # print(check.to_string())\n",
    "            initials = check['first name'].to_list()\n",
    "            iis = []\n",
    "            \n",
    "            #list of first names from leg_ref\n",
    "            for ii,jj in enumerate(initials):\n",
    "                if re.search(fr'^{initial}', str(jj)):\n",
    "                    iis.append(ii)\n",
    "            \n",
    "            #intials checked\n",
    "            printit = False\n",
    "            if len(iis) == 1:\n",
    "                df_i = iis[0]\n",
    "                full_pk = str(int(check.loc[df_i,'full_pk']))\n",
    "                v.loc[i,'full_pk'] = full_pk\n",
    "                if printit == True:\n",
    "                    print('narrowed it down')\n",
    "                    print('_________________')\n",
    "            elif len(iis) > 1:\n",
    "                if printit == True:\n",
    "                    print('more than 1 still')\n",
    "                    print('_________________')\n",
    "                break\n",
    "            \n",
    "            #no results from initials look up\n",
    "            else:\n",
    "                \n",
    "                print('no match on intials, looking in bio')\n",
    "                \n",
    "                #search website\n",
    "                route_2_url = bio_dict.get(j)\n",
    "                driver.get(route_2_url)\n",
    "                html_from_page = driver.page_source\n",
    "                bio_soup = BeautifulSoup(html_from_page, 'html.parser')\n",
    "\n",
    "                #retrieve info\n",
    "                title = bio_soup.find(\"h1\", {\"class\": \"section-title\"})\n",
    "                cl_title = re.sub(\"[Rr]ep[resentative]*|[Ss]en[ator]*\",'', title.text).replace('()','').strip()\n",
    "                fname = cl_title.split(' ', 1)[0]\n",
    "                print(f'first name: {fname}')\n",
    "                half = len(fname)//2\n",
    "                fname_pt1 = fname[:half]\n",
    "                fname_pt2 = fname[half:]\n",
    "                    \n",
    "                for ii,jj in enumerate(initials):\n",
    "                    if re.search(f'{fname_pt1}*{fname_pt2}*', str(jj)):\n",
    "                        print(f'found {jj}')\n",
    "                        full_pk = str(int(check.loc[ii,'full_pk']))\n",
    "                        v.loc[i,'full_pk'] = full_pk\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                # print(cl_title)\n",
    "                \n",
    "                \n",
    "                print(initials)\n",
    "                print(f'searched value: {name}')\n",
    "\n",
    "                print('_________________')\n",
    "        \n",
    "        \n",
    "        #this means there were no results from the check\n",
    "        else:\n",
    "            print('##')\n",
    "\n",
    "            print(\"backup\")\n",
    "            name2 = name.title()\n",
    "            check2 = leg_ref[leg_ref['last name'].str.contains(name2)].reset_index(drop=True)\n",
    "            print(f'name2: {name2}')\n",
    "            if len(check2) == 1:\n",
    "                full_pk = str(int(check2.loc[0,'full_pk']))\n",
    "                v.loc[i,'full_pk'] = full_pk\n",
    "                print(\"backup: sucess\")\n",
    "                print('_________________')\n",
    "            elif len(check2) == 0:\n",
    "                print(\"backup: fail [no results from 2nd check]\")\n",
    "                print('_________________')\n",
    "                print(j)\n",
    "                print(name)\n",
    "                print(check2.to_string())\n",
    "                break\n",
    "\n",
    "            elif len(check2) > 1:\n",
    "\n",
    "                bio_ref = bio_dict.get(j)\n",
    "                print(bio_ref)\n",
    "                print(\"backup: fail [still more than one]\")\n",
    "                print('_________________')\n",
    "                print(j)\n",
    "                print(name)\n",
    "                print(check2.to_string())\n",
    "                break\n",
    "\n",
    "\n",
    "    # check for nan values\n",
    "    no_values = v[v['full_pk'].isna()]\n",
    "    indexs = no_values.index.to_list()\n",
    "    if no_values.empty:\n",
    "        continue\n",
    "    else:\n",
    "        print('still something left in this one')\n",
    "        print(no_values.to_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final df creation\n",
    "nc_com_df = pd.concat(comm_info_dict.values()).reset_index(drop=True)\n",
    "pop_column = nc_com_df.pop('full_pk')\n",
    "\n",
    "\n",
    "# Insert 'col_B' at the beginning of the DataFrame (index 0)\n",
    "nc_com_df.insert(0, 'full_pk', pop_column)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hunt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
